{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://chbe.gatech.edu/directory/person/alex-abramson\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_content = '''\n",
    "<div about=\"/directory/person/alex-abramson\" class=\"node node--type-dir-person node--view-mode-teaser\">\n",
    "  <div class=\"gt-image\">\n",
    "    <a href=\"/directory/person/alex-abramson\">\n",
    "      <div class=\"field field--name-field-person-headshot field--type-image field--label-hidden\">\n",
    "        <div class=\"field__item\">\n",
    "          <img loading=\"lazy\" src=\"/sites/default/files/styles/profile_headshot_square_400x400_/public/profile/2023/08/Alex%20Abramsonweb.png.jpg?itok=WzCWvpe3\" width=\"400\" height=\"400\" alt=\"Alex Abramson\" typeof=\"foaf:Image\">\n",
    "        </div>\n",
    "      </div>\n",
    "    </a>\n",
    "  </div>\n",
    "  <a href=\"/directory/person/alex-abramson\" class=\"dir_link\">\n",
    "    <div class=\"field field--name-field-person-name field--type-name field--label-hidden\">\n",
    "      <div class=\"field__item\">Alex Abramson</div>\n",
    "    </div>\n",
    "  </a>\n",
    "  <div class=\"field field--name-field-person-job-title-s- field--type-string field--label-hidden\">\n",
    "    <div class=\"field__item\">Assistant Professor</div>\n",
    "  </div>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "# Base URL of the website\n",
    "base_url = \"https://chbe.gatech.edu\"\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the first <a> tag with an href attribute\n",
    "a_tag = soup.find('a', href=True)\n",
    "\n",
    "# Construct the absolute URL\n",
    "relative_url = a_tag['href']\n",
    "absolute_url = base_url + relative_url\n",
    "\n",
    "print(absolute_url)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T00:58:41.435168800Z",
     "start_time": "2024-07-09T00:58:41.389423Z"
    }
   },
   "id": "76ca2b90c62301bf",
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://chbe.gatech.edu/directory/person/alex-abramson\n",
      "https://chbe.gatech.edu/directory/person/alex-abramson\n",
      "https://chbe.gatech.edu/directory/person/saad-bhamla\n",
      "https://chbe.gatech.edu/directory/person/saad-bhamla\n",
      "https://chbe.gatech.edu/directory/person/john-blazeck\n",
      "https://chbe.gatech.edu/directory/person/john-blazeck\n",
      "https://chbe.gatech.edu/directory/person/andreas-bommarius\n",
      "https://chbe.gatech.edu/directory/person/andreas-bommarius\n",
      "https://chbe.gatech.edu/directory/person/fani-boukouvala\n",
      "https://chbe.gatech.edu/directory/person/fani-boukouvala\n",
      "https://chbe.gatech.edu/directory/person/victor-breedveld\n",
      "https://chbe.gatech.edu/directory/person/victor-breedveld\n",
      "https://chbe.gatech.edu/directory/person/blair-brettmann\n",
      "https://chbe.gatech.edu/directory/person/blair-brettmann\n",
      "https://chbe.gatech.edu/directory/person/julie-champion\n",
      "https://chbe.gatech.edu/directory/person/julie-champion\n",
      "https://chbe.gatech.edu/directory/person/lily-cheung\n",
      "https://chbe.gatech.edu/directory/person/lily-cheung\n",
      "https://chbe.gatech.edu/directory/person/christian-cuba-torres\n",
      "https://chbe.gatech.edu/directory/person/christian-cuba-torres\n",
      "https://chbe.gatech.edu/directory/person/michael-filler\n",
      "https://chbe.gatech.edu/directory/person/michael-filler\n",
      "https://chbe.gatech.edu/directory/person/david-flaherty\n",
      "https://chbe.gatech.edu/directory/person/david-flaherty\n",
      "https://chbe.gatech.edu/directory/person/thomas-fuller\n",
      "https://chbe.gatech.edu/directory/person/thomas-fuller\n",
      "https://chbe.gatech.edu/directory/person/benjamin-galfond\n",
      "https://chbe.gatech.edu/directory/person/benjamin-galfond\n",
      "https://chbe.gatech.edu/directory/person/martha-grover\n",
      "https://chbe.gatech.edu/directory/person/martha-grover\n",
      "https://chbe.gatech.edu/directory/person/marta-hatzell\n",
      "https://chbe.gatech.edu/directory/person/marta-hatzell\n",
      "https://chbe.gatech.edu/directory/person/yuhang-hu\n",
      "https://chbe.gatech.edu/directory/person/yuhang-hu\n",
      "https://chbe.gatech.edu/directory/person/vida-jamali\n",
      "https://chbe.gatech.edu/directory/person/vida-jamali\n",
      "https://chbe.gatech.edu/directory/person/christopher-jones\n",
      "https://chbe.gatech.edu/directory/person/christopher-jones\n",
      "https://chbe.gatech.edu/directory/person/ravi-kane\n",
      "https://chbe.gatech.edu/directory/person/ravi-kane\n",
      "https://chbe.gatech.edu/directory/person/paul-kohl\n",
      "https://chbe.gatech.edu/directory/person/paul-kohl\n",
      "https://chbe.gatech.edu/directory/person/william-koros\n",
      "https://chbe.gatech.edu/directory/person/william-koros\n",
      "https://chbe.gatech.edu/directory/person/nian-liu\n",
      "https://chbe.gatech.edu/directory/person/nian-liu\n",
      "https://chbe.gatech.edu/directory/person/ryan-lively\n",
      "https://chbe.gatech.edu/directory/person/ryan-lively\n",
      "https://chbe.gatech.edu/directory/person/hang-lu\n",
      "https://chbe.gatech.edu/directory/person/hang-lu\n",
      "https://chbe.gatech.edu/directory/person/peter-ludovice\n",
      "https://chbe.gatech.edu/directory/person/peter-ludovice\n",
      "https://chbe.gatech.edu/directory/person/martin-maldovan\n",
      "https://chbe.gatech.edu/directory/person/martin-maldovan\n",
      "https://chbe.gatech.edu/directory/person/andrew-medford\n",
      "https://chbe.gatech.edu/directory/person/andrew-medford\n",
      "https://chbe.gatech.edu/directory/person/j-carson-meredith\n",
      "https://chbe.gatech.edu/directory/person/j-carson-meredith\n",
      "https://chbe.gatech.edu/directory/person/sankar-nair\n",
      "https://chbe.gatech.edu/directory/person/sankar-nair\n",
      "https://chbe.gatech.edu/directory/person/nga-lee-sally-ng\n",
      "https://chbe.gatech.edu/directory/person/nga-lee-sally-ng\n",
      "https://chbe.gatech.edu/directory/person/solomon-tolulope-oyakhire\n",
      "https://chbe.gatech.edu/directory/person/solomon-tolulope-oyakhire\n",
      "https://chbe.gatech.edu/directory/person/anant-paravastu\n",
      "https://chbe.gatech.edu/directory/person/anant-paravastu\n",
      "https://chbe.gatech.edu/directory/person/pamela-peralta-yahya\n",
      "https://chbe.gatech.edu/directory/person/pamela-peralta-yahya\n",
      "https://chbe.gatech.edu/directory/person/mark-prausnitz\n",
      "https://chbe.gatech.edu/directory/person/mark-prausnitz\n",
      "https://chbe.gatech.edu/directory/person/matthew-realff\n",
      "https://chbe.gatech.edu/directory/person/matthew-realff\n",
      "https://chbe.gatech.edu/directory/person/nick-sahinidis\n",
      "https://chbe.gatech.edu/directory/person/nick-sahinidis\n",
      "https://chbe.gatech.edu/directory/person/joseph-scott\n",
      "https://chbe.gatech.edu/directory/person/joseph-scott\n",
      "https://chbe.gatech.edu/directory/person/david-sholl\n",
      "https://chbe.gatech.edu/directory/person/david-sholl\n",
      "https://chbe.gatech.edu/directory/person/carsten-sievers\n",
      "https://chbe.gatech.edu/directory/person/carsten-sievers\n",
      "https://chbe.gatech.edu/directory/person/jacqueline-snedeker\n",
      "https://chbe.gatech.edu/directory/person/jacqueline-snedeker\n",
      "https://chbe.gatech.edu/directory/person/natalie-stingelin\n",
      "https://chbe.gatech.edu/directory/person/natalie-stingelin\n",
      "https://chbe.gatech.edu/directory/person/mark-styczynski\n",
      "https://chbe.gatech.edu/directory/person/mark-styczynski\n",
      "https://chbe.gatech.edu/directory/person/yonathan-thio\n",
      "https://chbe.gatech.edu/directory/person/yonathan-thio\n",
      "https://chbe.gatech.edu/directory/person/zhaohui-julene-tong\n",
      "https://chbe.gatech.edu/directory/person/zhaohui-julene-tong\n",
      "https://chbe.gatech.edu/directory/person/krista-walton\n",
      "https://chbe.gatech.edu/directory/person/krista-walton\n",
      "https://chbe.gatech.edu/directory/person/corey-wilson\n",
      "https://chbe.gatech.edu/directory/person/corey-wilson\n",
      "https://chbe.gatech.edu/directory/person/micah-s-ziegler\n",
      "https://chbe.gatech.edu/directory/person/micah-s-ziegler\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the directory page\n",
    "url = \"https://chbe.gatech.edu/directory1?title=&field_person_category_target_id=1&field_person_chbe_dis_target_id=All\"\n",
    "\n",
    "# Send a GET request to fetch the HTML content\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Base URL of the website\n",
    "base_url = \"https://chbe.gatech.edu\"\n",
    "\n",
    "# Find all <a> tags with an href attribute containing '/directory/person/'\n",
    "a_tags = soup.find_all('a', href=True)\n",
    "person_links = []\n",
    "\n",
    "for a_tag in a_tags:\n",
    "    href = a_tag['href']\n",
    "    if '/directory/person/' in href:\n",
    "        # Construct the absolute URL and add to the list\n",
    "        absolute_url = base_url + href\n",
    "        person_links.append(absolute_url)\n",
    "\n",
    "# Print all extracted URLs\n",
    "for link in person_links:\n",
    "    print(link)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T00:59:31.135970Z",
     "start_time": "2024-07-09T00:59:29.756775300Z"
    }
   },
   "id": "5315501bffb3620e",
   "execution_count": 117
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Zhaohui (Julene) Tong\n",
      "Position: Associate Professor and James C. Barber Faculty Fellow\n",
      "Email: zt7@gatech.edu\n",
      "Research Focus: Materials and Nanotechnology\n",
      "Energy and Sustainability\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp\\ipykernel_7036\\3255825346.py:27: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  research_focus_tag = soup.find('div', class_='field__label', text='Disciplines')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the person's profile page\n",
    "url = \"https://chbe.gatech.edu/directory/person/zhaohui-julene-tong\"\n",
    "\n",
    "# Send a GET request to fetch the HTML content\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract the name\n",
    "name_tag = soup.find('div', id='gt-page-title')\n",
    "name = name_tag.find('span').text.strip()\n",
    "\n",
    "# Extract the position\n",
    "position_tag = soup.find('div', class_='field--name-field-person-job-title-s-')\n",
    "position = position_tag.find('div', class_='field__item').text.strip()\n",
    "\n",
    "# Extract the email\n",
    "email_tag = soup.find('div', class_='field--name-field-person-email')\n",
    "email = email_tag.find('a').text.strip()\n",
    "\n",
    "# Extract the research focus\n",
    "research_focus_tag = soup.find('div', class_='field__label', text='Disciplines')\n",
    "research_focus = research_focus_tag.find_next_sibling('div').text.strip()\n",
    "\n",
    "# Print the extracted information\n",
    "print(f\"Name: {name}\")\n",
    "print(f\"Position: {position}\")\n",
    "print(f\"Email: {email}\")\n",
    "print(f\"Research Focus: {research_focus}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T01:06:35.095659700Z",
     "start_time": "2024-07-09T01:06:34.011024100Z"
    }
   },
   "id": "8fd23043ecb8566b",
   "execution_count": 119
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Andreas Bommarius\n",
      "Position: Professor, Clifford W. Rackley Faculty Fellow, and Nobles Family Faculty Fellow\n",
      "Email: andreas.bommarius@chbe.gatech.edu\n",
      "Research Focus: Biotechnology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp\\ipykernel_7036\\2736814723.py:27: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  research_focus_tag = soup.find('div', class_='field__label h4', text='Disciplines')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the person's profile page\n",
    "url = \"https://chbe.gatech.edu/directory/person/andreas-bommarius\"\n",
    "\n",
    "# Send a GET request to fetch the HTML content\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract the name\n",
    "name_tag = soup.find('div', id='gt-page-title')\n",
    "name = name_tag.find('span').text.strip()\n",
    "\n",
    "# Extract the position\n",
    "position_tag = soup.find('div', class_='field--name-field-person-job-title-s-')\n",
    "position = position_tag.find('div', class_='field__item').text.strip()\n",
    "\n",
    "# Extract the email\n",
    "email_tag = soup.find('div', class_='field--name-field-person-email')\n",
    "email = email_tag.find('a').text.strip()\n",
    "\n",
    "# Extract the research focus\n",
    "research_focus_tag = soup.find('div', class_='field__label h4', text='Disciplines')\n",
    "research_focus_items = research_focus_tag.find_next_sibling('div').find_all('div', class_='field__item')\n",
    "research_focus = \", \".join([item.text.strip() for item in research_focus_items])\n",
    "\n",
    "# Print the extracted information\n",
    "print(f\"Name: {name}\")\n",
    "print(f\"Position: {position}\")\n",
    "print(f\"Email: {email}\")\n",
    "print(f\"Research Focus: {research_focus}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T01:07:01.358203Z",
     "start_time": "2024-07-09T01:07:00.090375100Z"
    }
   },
   "id": "f2948f0bc6cac552",
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.me.gatech.edu/faculty/abdel-khalik-0\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_content = '''\n",
    "<div class=\"faculty__user-wrapper\">\n",
    "    <a href=\"/faculty/abdel-khalik-0\" hreflang=\"en\"><img loading=\"lazy\" src=\"/sites/default/files/styles/faculty_image_thumbnail/public/abdel_khalik_7.jpg?itok=0xONPPHr\" width=\"101\" height=\"135\" alt=\"\" typeof=\"foaf:Image\"></a>\n",
    "\n",
    "    <div class=\"faculty__user-details\">\n",
    "        <div class=\"faculty-name\"><a href=\"/faculty/abdel-khalik-0\">Said I. Abdel-Khalik</a></div>\n",
    "        <div class=\"faculty-title\">\n",
    "            Professor Emeritus\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the <a> tag inside the faculty__user-wrapper div\n",
    "a_tag = soup.find('div', class_='faculty__user-wrapper').find('a', href=True)\n",
    "\n",
    "# Extract the relative URL from the href attribute\n",
    "relative_url = a_tag['href']\n",
    "\n",
    "# Base URL of the website\n",
    "base_url = \"https://www.me.gatech.edu\"\n",
    "\n",
    "# Construct the absolute URL\n",
    "absolute_url = base_url + relative_url\n",
    "\n",
    "print(absolute_url)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T01:15:11.860832300Z",
     "start_time": "2024-07-09T01:15:11.823125700Z"
    }
   },
   "id": "6254daaec5b8a945",
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to faculty_profiles.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# URL of the faculty listing page\n",
    "url = \"https://www.me.gatech.edu/faculty\"\n",
    "\n",
    "# Function to fetch URLs from the faculty listing page\n",
    "def fetch_urls_from_page(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all faculty__user-wrapper divs\n",
    "    faculty_divs = soup.find_all('div', class_='faculty__user-wrapper')\n",
    "    \n",
    "    links = []\n",
    "    base_url = \"https://www.me.gatech.edu\"\n",
    "    \n",
    "    for div in faculty_divs:\n",
    "        a_tag = div.find('a', href=True)\n",
    "        if a_tag:\n",
    "            relative_url = a_tag['href']\n",
    "            absolute_url = base_url + relative_url\n",
    "            links.append(absolute_url)\n",
    "    \n",
    "    return links\n",
    "\n",
    "# Fetch URLs from the faculty listing page\n",
    "all_links = fetch_urls_from_page(url)\n",
    "\n",
    "# Prepare the data for CSV\n",
    "data = []\n",
    "for link in all_links:\n",
    "    profile = {\n",
    "        'University': 'Georgia Tech',\n",
    "        'Department': 'Mechanical Engineering',\n",
    "        'Name': 'N/A',\n",
    "        'Position': 'N/A',\n",
    "        'Link': link,\n",
    "        'Email': 'N/A',\n",
    "        'Research Focus': 'N/A'\n",
    "    }\n",
    "    data.append(profile)\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file = \"faculty_profiles.csv\"\n",
    "\n",
    "# Specify the headers/column names\n",
    "headers = ['University', 'Department', 'Name', 'Position', 'Link', 'Email', 'Research Focus']\n",
    "\n",
    "# Write data to CSV\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Data has been saved to {csv_file}.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T01:15:46.751919900Z",
     "start_time": "2024-07-09T01:15:45.126070900Z"
    }
   },
   "id": "c737a3be62bd08c4",
   "execution_count": 123
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Said I. Abdel-Khalik\n",
      "Position: Professor Emeritus\n",
      "Email: said.abdelkhalik@me.gatech.edu\n",
      "Research Focus: Not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp\\ipykernel_7036\\4291044663.py:32: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  research_focus_tag = soup.find('h5', text='Research Area')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the person's profile page\n",
    "url = \"https://www.me.gatech.edu/faculty/abdel-khalik-0\"\n",
    "\n",
    "# Function to scrape the data from the given URL using requests and BeautifulSoup\n",
    "def scrape_data(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    html_content = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract the name\n",
    "    name_tag = soup.find('h2', class_='display-4')\n",
    "    name = name_tag.text.strip() if name_tag else 'Not found'\n",
    "\n",
    "    # Extract the position\n",
    "    position_tag = soup.find('div', class_='field--name-body').find('h4')\n",
    "    position = position_tag.text.strip() if position_tag else 'Not found'\n",
    "\n",
    "    # Extract the email\n",
    "    email_tag = soup.find('a', href=lambda href: href and \"mailto\" in href)\n",
    "    email = email_tag.text.strip() if email_tag else 'Not found'\n",
    "\n",
    "    # Extract the research focus\n",
    "    research_focus_tag = soup.find('h5', text='Research Area')\n",
    "    research_focus = research_focus_tag.find_next_sibling('strong').text.strip() if research_focus_tag else 'Not found'\n",
    "\n",
    "    return {\n",
    "        \"Name\": name,\n",
    "        \"Position\": position,\n",
    "        \"Email\": email,\n",
    "        \"Research Focus\": research_focus\n",
    "    }\n",
    "\n",
    "# Scrape data from the URL\n",
    "scraped_data = scrape_data(url)\n",
    "\n",
    "# Print the extracted information\n",
    "print(f\"Name: {scraped_data['Name']}\")\n",
    "print(f\"Position: {scraped_data['Position']}\")\n",
    "print(f\"Email: {scraped_data['Email']}\")\n",
    "print(f\"Research Focus: {scraped_data['Research Focus']}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T01:31:14.277628100Z",
     "start_time": "2024-07-09T01:31:13.168749300Z"
    }
   },
   "id": "7c31c97fd0c6ee1",
   "execution_count": 130
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Area: Nuclear & Radiological Engineering\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '''\n",
    "<div class=\"banner-text-wrapper black col-md-12 col-lg-5 order-2 order-lg-1 p-5\">\n",
    "    <h2 class=\"display-4\">Said I. Abdel-Khalik</h2>\n",
    "    <div class=\"field field--name-body field--type-text-with-summary field--label-hidden field__item\">\n",
    "        <h4>Professor Emeritus</h4>\n",
    "        <p><strong>Location:</strong> Love, Room 324<br><strong>Email:</strong> <a href=\"mailto: said.abdelkhalik@me.gatech.edu\">said.abdelkhalik@me.gatech.edu</a><br><strong>Telephone:</strong> <a href=\"tel:404.894.3719\">404.894.3719</a><br><strong>Fax:</strong> 404.894.8503</p>\n",
    "        <p>&nbsp;</p>\n",
    "        <h5>Research Area: Nuclear &amp; Radiological Engineering</h5>\n",
    "    </div>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Find the <h5> tag within the specified <div> class\n",
    "research_area_tag = soup.find('div', class_='field__item').find('h5')\n",
    "\n",
    "# Extract the text inside the <h5> tag\n",
    "research_area = research_area_tag.text.strip()\n",
    "\n",
    "print(research_area)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T01:36:30.910380900Z",
     "start_time": "2024-07-09T01:36:30.716668700Z"
    }
   },
   "id": "80be7919706385cf",
   "execution_count": 131
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Area: Nuclear & Radiological Engineering\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage\n",
    "url = 'https://www.me.gatech.edu/faculty/abdel-khalik-0'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the <div> with class 'field__item' that contains the research area information\n",
    "research_area_div = soup.find('div', class_='field__item')\n",
    "\n",
    "# Find the <h5> tag within the research area <div>\n",
    "research_area_tag = research_area_div.find('h5')\n",
    "\n",
    "# Extract the text inside the <h5> tag\n",
    "research_area = research_area_tag.text.strip()\n",
    "\n",
    "print(research_area)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T01:37:00.456871500Z",
     "start_time": "2024-07-09T01:36:58.812373200Z"
    }
   },
   "id": "7c14ce642cbd06e",
   "execution_count": 132
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping and CSV update complete. Data saved to XXx.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape the data from the given URL using requests and BeautifulSoup\n",
    "def scrape_data(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    html_content = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract the name\n",
    "    name = soup.find('h2', class_='display-4').text.strip() if soup.find('h2', class_='display-4') else 'Not found'\n",
    "\n",
    "    # Extract the position\n",
    "    position_tag = soup.find('div', class_='field--name-body').find('h4')\n",
    "    position = position_tag.text.strip() if position_tag else 'Not found'\n",
    "\n",
    "    # Extract the email\n",
    "    email_tag = soup.find('a', href=lambda href: href and \"mailto\" in href)\n",
    "    email = email_tag.text.strip() if email_tag else 'Not found'\n",
    "\n",
    "    # Extract the research focus\n",
    "    research_area_div = soup.find('div', class_='field__item')\n",
    "\n",
    "    # Find the <h5> tag within the research area <div>\n",
    "    research_area_tag = research_area_div.find('h5')\n",
    "    \n",
    "    research_focus = research_area_tag.text.strip() if research_area_tag else 'Not found'\n",
    "\n",
    "    university = 'GeorgiaTech'\n",
    "    department = 'Mechanical Engineering'\n",
    "\n",
    "    return {\n",
    "        \"Name\": name,\n",
    "        \"Position\": position,\n",
    "        \"Email\": email,\n",
    "        \"Research Focus\": research_focus,\n",
    "        \"Department\": department\n",
    "    }\n",
    "\n",
    "# Path to the CSV file (considering the use of raw string for the path)\n",
    "csv_file = r'D:\\Files\\Upwork\\Scrape\\Us_30_Uni_engineering\\Src\\3_GeorgiaTech\\MechanicalEngineering\\faculty_profiles.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.head(5)\n",
    "# Initialize new columns in the DataFrame\n",
    "df['University'] = ''\n",
    "df['Department'] = ''\n",
    "df['Name'] = ''\n",
    "df['Position'] = ''\n",
    "\n",
    "df['Email'] = ''\n",
    "df['Research Focus'] = ''\n",
    "\n",
    "\n",
    "# Loop through URLs and scrape data\n",
    "for index, row in df.iterrows():\n",
    "    url = row['Link']  # Assuming the column containing URLs is named 'Link'\n",
    "    scraped_data = scrape_data(url)\n",
    "    df.at[index, 'Name'] = scraped_data[\"Name\"]\n",
    "    df.at[index, 'Position'] = scraped_data[\"Position\"]\n",
    "    df.at[index, 'Email'] = scraped_data[\"Email\"]\n",
    "    df.at[index, 'Research Focus'] = scraped_data[\"Research Focus\"]\n",
    "    df.at[index, 'Department'] = scraped_data[\"Department\"]\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_csv_file = r'XXx.csv'\n",
    "\n",
    "# Specify the headers/column names\n",
    "headers = ['Name', 'Position', 'Email', 'Research Focus', 'Department']\n",
    "\n",
    "# Write data to CSV\n",
    "df.to_csv(output_csv_file, index=False, columns=headers)\n",
    "\n",
    "print(f\"Scraping and CSV update complete. Data saved to {output_csv_file}.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T02:14:39.700743900Z",
     "start_time": "2024-07-09T02:14:34.513353200Z"
    }
   },
   "id": "5394b3a4e0668662",
   "execution_count": 142
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping and CSV update complete. Data saved to ooo.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape the data from the given URL using requests and BeautifulSoup\n",
    "def scrape_data(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    html_content = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract the name\n",
    "    name = soup.find('h2', class_='display-4').text.strip() if soup.find('h2', class_='display-4') else 'Not found'\n",
    "\n",
    "    # Extract the position\n",
    "    position_tag = soup.find('div', class_='field--name-body').find('h4')\n",
    "    position = position_tag.text.strip() if position_tag else 'Not found'\n",
    "\n",
    "    # Extract the email\n",
    "    email_tag = soup.find('a', href=lambda href: href and \"mailto\" in href)\n",
    "    email = email_tag.text.strip() if email_tag else 'Not found'\n",
    "\n",
    "    # Extract the research focus\n",
    "    research_area_div = soup.find('div', class_='field__item')\n",
    "    research_area_tag = research_area_div.find('h5')\n",
    "    research_focus = research_area_tag.text.strip() if research_area_tag else 'Not found'\n",
    "\n",
    "    university = 'GeorgiaTech'\n",
    "    department = 'Mechanical Engineering'\n",
    "\n",
    "    return {\n",
    "        \"University\": university,\n",
    "        \"Department\": department,\n",
    "        \"Name\": name,\n",
    "        \"Position\": position,\n",
    "        \"Email\": email,\n",
    "        \"Research Focus\": research_focus\n",
    "        \n",
    "    }\n",
    "\n",
    "# Path to the CSV file containing URLs\n",
    "csv_file = r'D:\\Files\\Upwork\\Scrape\\Us_30_Uni_engineering\\Src\\3_GeorgiaTech\\MechanicalEngineering\\faculty_profiles.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.head(5)\n",
    "# Initialize new columns in the DataFrame\n",
    "df['University'] = ''\n",
    "df['Department'] = ''\n",
    "df['Name'] = ''\n",
    "df['Position'] = ''\n",
    "df['Email'] = ''\n",
    "df['Research Focus'] = ''\n",
    "\n",
    "# Loop through URLs and scrape data\n",
    "for index, row in df.iterrows():\n",
    "    url = row['Link']  # Assuming the column containing URLs is named 'Link'\n",
    "    scraped_data = scrape_data(url)\n",
    "    df.at[index, 'Name'] = scraped_data[\"Name\"]\n",
    "    df.at[index, 'Position'] = scraped_data[\"Position\"]\n",
    "    df.at[index, 'Email'] = scraped_data[\"Email\"]\n",
    "    df.at[index, 'Research Focus'] = scraped_data[\"Research Focus\"]\n",
    "    df.at[index, 'Department'] = scraped_data[\"Department\"]\n",
    "    df.at[index, 'University'] = scraped_data[\"University\"]\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_csv_file = r'ooo.csv'\n",
    "\n",
    "# Specify the headers/column names\n",
    "headers = ['University', 'Department', 'Name', 'Position', 'Link', 'Email', 'Research Focus']\n",
    "\n",
    "# Write data to CSV\n",
    "df.to_csv(output_csv_file, index=False, columns=headers)\n",
    "\n",
    "print(f\"Scraping and CSV update complete. Data saved to {output_csv_file}.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T02:21:31.625716300Z",
     "start_time": "2024-07-09T02:21:26.200935400Z"
    }
   },
   "id": "41be3926c4986f17",
   "execution_count": 147
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp\\ipykernel_7036\\3296206107.py:29: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  research_tag = soup.find('h5', text='Research Area')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping and CSV update complete. Data saved to rrr.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape the data from the given URL using requests and BeautifulSoup\n",
    "def scrape_data(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    html_content = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract the name\n",
    "    name = soup.find('h2', class_='display-4').text.strip() if soup.find('h2', class_='display-4') else 'Not found'\n",
    "\n",
    "    # Extract the position\n",
    "    position_tag = soup.find('div', class_='field--name-body').find('h4')\n",
    "    position = position_tag.text.strip() if position_tag else 'Not found'\n",
    "\n",
    "    # Extract the email\n",
    "    email_tag = soup.find('a', href=lambda href: href and \"mailto\" in href)\n",
    "    email = email_tag.text.strip() if email_tag else 'Not found'\n",
    "\n",
    "    # Extract the research focus\n",
    "    research_tag = soup.find('h5', text='Research Area')\n",
    "    research_focus = research_tag.find_next_sibling('strong').text.strip() if research_tag else 'Not found'\n",
    "\n",
    "    university = 'GeorgiaTech'\n",
    "    department = 'Mechanical Engineering'\n",
    "\n",
    "    return {\n",
    "        \"Name\": name,\n",
    "        \"Position\": position,\n",
    "        \"Email\": email,\n",
    "        \"Research Focus\": research_focus,\n",
    "        \"Department\": department\n",
    "    }\n",
    "\n",
    "# Path to the CSV file (considering the use of raw string for the path)\n",
    "csv_file = r'D:\\Files\\Upwork\\Scrape\\Us_30_Uni_engineering\\Src\\3_GeorgiaTech\\MechanicalEngineering\\faculty_profiles.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.head(5)\n",
    "# Initialize new columns in the DataFrame\n",
    "df['Name'] = ''\n",
    "df['Position'] = ''\n",
    "df['Email'] = ''\n",
    "df['Research Focus'] = ''\n",
    "df['Department'] = ''\n",
    "\n",
    "# Loop through URLs and scrape data\n",
    "for index, row in df.iterrows():\n",
    "    url = row['Link']  # Assuming the column containing URLs is named 'Link'\n",
    "    scraped_data = scrape_data(url)\n",
    "    df.at[index, 'Name'] = scraped_data[\"Name\"]\n",
    "    df.at[index, 'Position'] = scraped_data[\"Position\"]\n",
    "    df.at[index, 'Email'] = scraped_data[\"Email\"]\n",
    "    df.at[index, 'Research Focus'] = scraped_data[\"Research Focus\"]\n",
    "    df.at[index, 'Department'] = scraped_data[\"Department\"]\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_csv_file = 'rrr.csv'\n",
    "\n",
    "# Specify the headers/column names\n",
    "headers = ['University', 'Department','Name', 'Position','Link',  'Email', 'Research Focus']\n",
    "\n",
    "# Write data to CSV\n",
    "df.to_csv(output_csv_file, index=False, columns=headers)\n",
    "\n",
    "print(f\"Scraping and CSV update complete. Data saved to {output_csv_file}.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T02:00:43.362799800Z",
     "start_time": "2024-07-09T02:00:37.807591600Z"
    }
   },
   "id": "11e500fb66e4ec3e",
   "execution_count": 136
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping and CSV update complete. Data saved to mmm.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape the data from the given URL using requests and BeautifulSoup\n",
    "def scrape_data(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    html_content = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract the name\n",
    "    name = soup.find('h2', class_='display-4').text.strip() if soup.find('h2', class_='display-4') else 'Not found'\n",
    "\n",
    "    # Extract the position\n",
    "    position_tag = soup.find('div', class_='field--name-body').find('h4')\n",
    "    position = position_tag.text.strip() if position_tag else 'Not found'\n",
    "\n",
    "    # Extract the email\n",
    "    email_tag = soup.find('a', href=lambda href: href and \"mailto\" in href)\n",
    "    email = email_tag.text.strip() if email_tag else 'Not found'\n",
    "\n",
    "    # Extract the research focus\n",
    "    research_focus = []\n",
    "    research_tags = soup.find_all('div', class_='field__item')\n",
    "    for tag in research_tags:\n",
    "        a_tag = tag.find('a', href=True)\n",
    "        if a_tag:\n",
    "            href = a_tag['href']\n",
    "            if \"/research-area/\" in href:\n",
    "                research_focus.append(tag.text.strip())\n",
    "\n",
    "    if not research_focus:\n",
    "        research_focus = 'Not found'\n",
    "    else:\n",
    "        research_focus = ', '.join(research_focus)\n",
    "\n",
    "    university = 'GeorgiaTech'\n",
    "    department = 'Material Science Engineering'\n",
    "\n",
    "    return {\n",
    "        \"Name\": name,\n",
    "        \"Position\": position,\n",
    "        \"Email\": email,\n",
    "        \"Research Focus\": research_focus,\n",
    "        \"Department\": department\n",
    "    }\n",
    "\n",
    "# Path to the CSV file (considering the use of raw string for the path)\n",
    "csv_file = r'D:\\Files\\Upwork\\Scrape\\Us_30_Uni_engineering\\Src\\3_GeorgiaTech\\MaterialsScienceEngineering\\x.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.head(10)  # Just for testing with first 10 rows\n",
    "df['University'] = 'GeorgiaTech'\n",
    "df['Department'] = 'Material Science Engineering'\n",
    "\n",
    "# Initialize new columns in the DataFrame\n",
    "df['Name'] = ''\n",
    "df['Position'] = ''\n",
    "df['Email'] = ''\n",
    "df['Research Focus'] = ''\n",
    "\n",
    "# Loop through URLs and scrape data\n",
    "for index, row in df.iterrows():\n",
    "    url = row['Link']  # Assuming the column containing URLs is named 'Link'\n",
    "    scraped_data = scrape_data(url)\n",
    "    df.at[index, 'Name'] = scraped_data[\"Name\"]\n",
    "    df.at[index, 'Position'] = scraped_data[\"Position\"]\n",
    "    df.at[index, 'Email'] = scraped_data[\"Email\"]\n",
    "    df.at[index, 'Research Focus'] = scraped_data[\"Research Focus\"]\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_csv_file = r'mmm.csv'\n",
    "\n",
    "# Specify the headers/column names\n",
    "headers = ['University', 'Department', 'Name', 'Position', 'Link', 'Email', 'Research Focus']\n",
    "\n",
    "# Write data to CSV\n",
    "df.to_csv(output_csv_file, index=False, columns=headers)\n",
    "\n",
    "print(f\"Scraping and CSV update complete. Data saved to {output_csv_file}.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T02:09:40.921913900Z",
     "start_time": "2024-07-09T02:09:28.112481Z"
    }
   },
   "id": "e0a34496d5dca16a",
   "execution_count": 141
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp\\ipykernel_7036\\3531886995.py:30: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  research_area_tag = research_area_div.find('h5', text='Research Area')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping and CSV update complete. Data saved to fff.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape the data from the given URL using requests and BeautifulSoup\n",
    "def scrape_data(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    html_content = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract the name\n",
    "    name = soup.find('h2', class_='display-4').text.strip() if soup.find('h2', class_='display-4') else 'Not found'\n",
    "\n",
    "    # Extract the position\n",
    "    position_tag = soup.find('div', class_='field--name-body').find('h4')\n",
    "    position = position_tag.text.strip() if position_tag else 'Not found'\n",
    "\n",
    "    # Extract the email\n",
    "    email_tag = soup.find('a', href=lambda href: href and \"mailto\" in href)\n",
    "    email = email_tag.text.strip() if email_tag else 'Not found'\n",
    "\n",
    "    # Extract the research focus\n",
    "    research_area_div = soup.find('div', class_='field__item')\n",
    "    research_area_tag = research_area_div.find('h5', text='Research Area')\n",
    "\n",
    "    if research_area_tag:\n",
    "        research_focus = research_area_tag.find_next_sibling('strong').text.strip()\n",
    "    else:\n",
    "        research_focus = 'Not found'\n",
    "\n",
    "    university = 'GeorgiaTech'\n",
    "    department = 'Mechanical Engineering'\n",
    "\n",
    "    return {\n",
    "        \"University\": university,\n",
    "        \"Department\": department,\n",
    "        \"Name\": name,\n",
    "        \"Position\": position,\n",
    "        \"Email\": email,\n",
    "        \"Research Focus\": research_focus\n",
    "    }\n",
    "\n",
    "# Path to the CSV file containing URLs\n",
    "csv_file = r'D:\\Files\\Upwork\\Scrape\\Us_30_Uni_engineering\\Src\\3_GeorgiaTech\\MechanicalEngineering\\faculty_profiles.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.head(5)\n",
    "\n",
    "# Initialize new columns in the DataFrame\n",
    "df['University'] = ''\n",
    "df['Department'] = ''\n",
    "df['Name'] = ''\n",
    "df['Position'] = ''\n",
    "df['Email'] = ''\n",
    "df['Research Focus'] = ''\n",
    "\n",
    "# Loop through URLs and scrape data\n",
    "for index, row in df.iterrows():\n",
    "    url = row['Link']  # Assuming the column containing URLs is named 'Link'\n",
    "    scraped_data = scrape_data(url)\n",
    "    df.at[index, 'University'] = scraped_data[\"University\"]\n",
    "    df.at[index, 'Department'] = scraped_data[\"Department\"]\n",
    "    df.at[index, 'Name'] = scraped_data[\"Name\"]\n",
    "    df.at[index, 'Position'] = scraped_data[\"Position\"]\n",
    "    df.at[index, 'Email'] = scraped_data[\"Email\"]\n",
    "    df.at[index, 'Research Focus'] = scraped_data[\"Research Focus\"]\n",
    "\n",
    "# Define the output CSV file path\n",
    "# output_csv_file = r'D:\\Files\\Upwork\\Scrape\\Us_30_Uni_engineering\\Result\\3_GeorgiaTech\\GeorgiaTech_MechanicalEngineering.csv'\n",
    "output_csv_file = 'fff.csv'\n",
    "# Specify the headers/column names\n",
    "headers = ['University', 'Department', 'Name', 'Position', 'Link', 'Email', 'Research Focus']\n",
    "\n",
    "# Write data to CSV\n",
    "df.to_csv(output_csv_file, index=False, columns=headers)\n",
    "\n",
    "print(f\"Scraping and CSV update complete. Data saved to {output_csv_file}.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T02:30:33.632231400Z",
     "start_time": "2024-07-09T02:30:28.340282400Z"
    }
   },
   "id": "e2b52ef642653084",
   "execution_count": 149
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp\\ipykernel_7036\\4270121377.py:30: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  research_tag = soup.find('h5', text='Research Area')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping and CSV update complete. Data saved to fff.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape the data from the given URL using requests and BeautifulSoup\n",
    "def scrape_data(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    html_content = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract the name\n",
    "    name = soup.find('h2', class_='display-4').text.strip() if soup.find('h2', class_='display-4') else 'Not found'\n",
    "\n",
    "    # Extract the position\n",
    "    position_tag = soup.find('div', class_='field--name-body').find('h4')\n",
    "    position = position_tag.text.strip() if position_tag else 'Not found'\n",
    "\n",
    "    # Extract the email\n",
    "    email_tag = soup.find('a', href=lambda href: href and \"mailto\" in href)\n",
    "    email = email_tag.text.strip() if email_tag else 'Not found'\n",
    "\n",
    "    # Extract the research focus\n",
    "    research_tag = soup.find('h5', text='Research Area')\n",
    "    research_focus = research_tag.find_next_sibling('strong').text.strip() if research_tag else 'Not found'\n",
    "\n",
    "    university = 'GeorgiaTech'\n",
    "    department = 'Mechanical Engineering'\n",
    "\n",
    "    return {\n",
    "        \"Name\": name,\n",
    "        \"Position\": position,\n",
    "        \"Email\": email,\n",
    "        \"Research Focus\": research_focus,\n",
    "        \"Department\": department\n",
    "    }\n",
    "\n",
    "# Path to the CSV file (considering the use of raw string for the path)\n",
    "csv_file = r'D:\\Files\\Upwork\\Scrape\\Us_30_Uni_engineering\\Src\\3_GeorgiaTech\\MaterialsScienceEngineering\\x.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.head(10)\n",
    "# Initialize new columns in the DataFrame\n",
    "df['Name'] = ''\n",
    "df['Position'] = ''\n",
    "df['Email'] = ''\n",
    "df['Research Focus'] = ''\n",
    "df['University'] = 'GeorgiaTech'\n",
    "df['Department'] = 'Material Science Engineering'\n",
    "\n",
    "# Loop through URLs and scrape data\n",
    "for index, row in df.iterrows():\n",
    "    url = row['Link']  # Assuming the column containing URLs is named 'Link'\n",
    "    scraped_data = scrape_data(url)\n",
    "    df.at[index, 'Name'] = scraped_data[\"Name\"]\n",
    "    df.at[index, 'Position'] = scraped_data[\"Position\"]\n",
    "    df.at[index, 'Email'] = scraped_data[\"Email\"]\n",
    "    df.at[index, 'Research Focus'] = scraped_data[\"Research Focus\"]\n",
    "\n",
    "# Define the output CSV file path\n",
    "# output_csv_file = r'zzz.csv'\n",
    "\n",
    "# Specify the headers/column names\n",
    "headers = ['University', 'Department', 'Name', 'Position', 'Link', 'Email', 'Research Focus']\n",
    "\n",
    "# Write data to CSV\n",
    "df.to_csv(output_csv_file, index=False, columns=headers)\n",
    "\n",
    "print(f\"Scraping and CSV update complete. Data saved to {output_csv_file}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T02:36:40.639990100Z",
     "start_time": "2024-07-09T02:36:27.962858900Z"
    }
   },
   "id": "5aab88a428dfe549",
   "execution_count": 150
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "21b070f91d19b2b0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
