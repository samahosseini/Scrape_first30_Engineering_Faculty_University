{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://vcresearch.berkeley.edu/faculty/alice-m-agogino\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Sample HTML input\n",
    "html_input = '''\n",
    "<a href=\"/faculty/alice-m-agogino\" class=\"field--name-field-name\"><span class=\"field field--name-title field--type-string field--label-hidden\">Alice M. Agogino</span>\n",
    "</a>\n",
    "'''\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://vcresearch.berkeley.edu\"\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html_input, 'html.parser')\n",
    "\n",
    "# Find the anchor tag and extract the href attribute\n",
    "anchor_tag = soup.find('a', class_='field--name-field-name')\n",
    "relative_url = anchor_tag['href']\n",
    "\n",
    "# Construct the full URL\n",
    "full_url = base_url + relative_url\n",
    "\n",
    "print(full_url)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-10T15:03:46.991040300Z",
     "start_time": "2024-07-10T15:03:46.977950900Z"
    }
   },
   "id": "1f19dd9c243c8802",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 33\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m# Example usage:\u001B[39;00m\n\u001B[0;32m     32\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://vcresearch.berkeley.edu/faculty/alice-m-agogino\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 33\u001B[0m links \u001B[38;5;241m=\u001B[39m \u001B[43mextract_links\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m link \u001B[38;5;129;01min\u001B[39;00m links:\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28mprint\u001B[39m(link)\n",
      "Cell \u001B[1;32mIn[14], line 21\u001B[0m, in \u001B[0;36mextract_links\u001B[1;34m(url)\u001B[0m\n\u001B[0;32m     19\u001B[0m extracted_links \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m link \u001B[38;5;129;01min\u001B[39;00m links:\n\u001B[1;32m---> 21\u001B[0m     href \u001B[38;5;241m=\u001B[39m \u001B[43mlink\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhref\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     22\u001B[0m     full_url \u001B[38;5;241m=\u001B[39m urljoin(url, href)  \u001B[38;5;66;03m# Convert relative URLs to absolute URLs\u001B[39;00m\n\u001B[0;32m     23\u001B[0m     extracted_links\u001B[38;5;241m.\u001B[39mappend(full_url)\n",
      "\u001B[1;31mTypeError\u001B[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def extract_links(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        # Parse the HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('div', class_=\"views-field views-field-rendered-entity\")\n",
    "\n",
    "        # Find all anchor tags\n",
    "        links = soup.find('a', href=True)\n",
    "\n",
    "        # Extract and print all href attributes (links)\n",
    "        extracted_links = []\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            full_url = urljoin(url, href)  # Convert relative URLs to absolute URLs\n",
    "            extracted_links.append(full_url)\n",
    "\n",
    "        return extracted_links\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://vcresearch.berkeley.edu/faculty/alice-m-agogino\"\n",
    "links = extract_links(url)\n",
    "for link in links:\n",
    "    print(link)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-10T15:12:36.878858Z",
     "start_time": "2024-07-10T15:12:35.674178200Z"
    }
   },
   "id": "c011ad55293f2058",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted URL: /faculty/pieter-abbeel\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_url(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        # Parse the HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the span tag with class 'field--name-title'\n",
    "        span_tag = soup.find('span', class_='field--name-title')\n",
    "\n",
    "        if span_tag:\n",
    "            # Find the parent <a> tag\n",
    "            anchor_tag = span_tag.parent\n",
    "\n",
    "            if anchor_tag and anchor_tag.name == 'a' and anchor_tag.has_attr('href'):\n",
    "                href_attr = anchor_tag['href']\n",
    "                \n",
    "                # Check if href_attr starts with '/faculty/'\n",
    "                if href_attr.startswith('/faculty/'):\n",
    "                    return href_attr.strip()  # Return the extracted URL path\n",
    "\n",
    "        return None  # Return None if no URL is found\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://vcresearch.berkeley.edu/faculty-expertise\"\n",
    "extracted_url = extract_url(url)\n",
    "if extracted_url:\n",
    "    print(f\"Extracted URL: {extracted_url}\")\n",
    "else:\n",
    "    print(\"URL extraction failed.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-10T15:28:05.222294200Z",
     "start_time": "2024-07-10T15:28:03.877985800Z"
    }
   },
   "id": "a397c5b889e0b262",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted URL: /faculty/pieter-abbeel\n",
      "Extracted URL: /faculty/marie-abe\n",
      "Extracted URL: /faculty/brooks-abel\n",
      "Extracted URL: /faculty/elizabeth-abel\n",
      "Extracted URL: /faculty/rebecca-abergel\n",
      "Extracted URL: /faculty/dor-abrahamson\n",
      "Extracted URL: /faculty/barbara-abrams\n",
      "Extracted URL: /faculty/kathryn-abrams\n",
      "Extracted URL: /faculty/charisma-acey\n",
      "Extracted URL: /faculty/david-ackerly\n",
      "Extracted URL: /faculty/hillel-adesnik\n",
      "Extracted URL: /faculty/ilan-adler\n",
      "Extracted URL: /faculty/mina-aganagic\n",
      "Extracted URL: /faculty/sabrina-agarwal\n",
      "Extracted URL: /faculty/vinod-aggarwal\n",
      "Extracted URL: /faculty/alice-m-agogino\n",
      "Extracted URL: /faculty/ian-agol\n",
      "Extracted URL: /faculty/adrian-aguilera\n",
      "Extracted URL: /faculty/jennifer-ahern\n",
      "Extracted URL: /faculty/wali-ahmadi\n",
      "Extracted URL: /faculty/asad-q-ahmed\n",
      "Extracted URL: /faculty/ashok-ajoy\n",
      "Extracted URL: /faculty/george-akerlof\n",
      "Extracted URL: /faculty/zakaria-al-balushi\n",
      "Extracted URL: /faculty/ahmed-alaa\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_urls(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        # Parse the HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all span tags with class 'field--name-title'\n",
    "        span_tags = soup.find_all('span', class_='field--name-title')\n",
    "\n",
    "        extracted_urls = []\n",
    "\n",
    "        for span_tag in span_tags:\n",
    "            # Find the parent <a> tag\n",
    "            anchor_tag = span_tag.parent\n",
    "\n",
    "            if anchor_tag and anchor_tag.name == 'a' and anchor_tag.has_attr('href'):\n",
    "                href_attr = anchor_tag['href']\n",
    "\n",
    "                # Check if href_attr starts with '/faculty/'\n",
    "                if href_attr.startswith('/faculty/'):\n",
    "                    extracted_urls.append(href_attr.strip())  # Append the extracted URL path\n",
    "\n",
    "        return extracted_urls\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://vcresearch.berkeley.edu/faculty-expertise\"\n",
    "extracted_urls = extract_urls(url)\n",
    "\n",
    "if extracted_urls:\n",
    "    for url in extracted_urls:\n",
    "        print(f\"Extracted URL: {url}\")\n",
    "else:\n",
    "    print(\"URL extraction failed or no URLs found.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-10T15:28:59.684079800Z",
     "start_time": "2024-07-10T15:28:58.176393400Z"
    }
   },
   "id": "d08d215726828c4a",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to profiles_urls.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import csv\n",
    "\n",
    "def fetch_urls_from_page(url):\n",
    "    links = []\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all span tags with class 'field--name-title'\n",
    "            span_tags = soup.find_all('span', class_='field--name-title')\n",
    "\n",
    "            for span_tag in span_tags:\n",
    "                # Find the parent <a> tag\n",
    "                anchor_tag = span_tag.parent\n",
    "\n",
    "                if anchor_tag and anchor_tag.name == 'a' and anchor_tag.has_attr('href'):\n",
    "                    href_attr = anchor_tag['href']\n",
    "\n",
    "                    # Check if href_attr starts with '/faculty/'\n",
    "                    if href_attr.startswith('/faculty/'):\n",
    "                        full_url = urljoin(url, href_attr)\n",
    "                        links.append(full_url)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "    return links\n",
    "\n",
    "# Example usage:\n",
    "directory_url = \"https://vcresearch.berkeley.edu/faculty-expertise\"\n",
    "all_links = fetch_urls_from_page(directory_url)\n",
    "\n",
    "# Prepare the data for CSV\n",
    "data = []\n",
    "for link in all_links:\n",
    "    profile = {\n",
    "        'University': 'Berkeley',\n",
    "        'Department': 'N/A',\n",
    "        'Name': 'N/A',\n",
    "        'Position': 'N/A',\n",
    "        'Link': link,\n",
    "        'Email': 'N/A',\n",
    "        'Research Focus': 'N/A'\n",
    "    }\n",
    "    data.append(profile)\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file = \"profiles_urls.csv\"\n",
    "\n",
    "# Specify the headers/column names\n",
    "headers = ['University', 'Department', 'Name', 'Position', 'Link', 'Email', 'Research Focus']\n",
    "\n",
    "# Write data to CSV\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Data has been saved to {csv_file}.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-10T16:17:49.887237Z",
     "start_time": "2024-07-10T16:17:46.153235100Z"
    }
   },
   "id": "b4c5f5956de437dd",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email: dma@me.berkeley.edu\n",
      "Position: Professor of the Graduate School\n",
      "Research Expertise: control systems,                              simulation,                              mechatronics,                              real time software,                              energy management,                              satellite attitude control,                              demand response,                              machine control\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp\\ipykernel_11372\\396052005.py:22: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  expertise_heading = soup.find('h2', text='Research Expertise and Interest')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_faculty_info(url):\n",
    "    # Fetch the HTML content from the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Extract email\n",
    "    email_tag = soup.find('a', href=lambda x: x and x.startswith('mailto:'))\n",
    "    email = email_tag.get_text(strip=True) if email_tag else \"\"\n",
    "\n",
    "    # Extract position\n",
    "    position_tag = soup.find('p', class_='large')\n",
    "    position = position_tag.get_text(strip=True) if position_tag else \"\"\n",
    "\n",
    "    # Extract research expertise\n",
    "    expertise_heading = soup.find('h2', text='Research Expertise and Interest')\n",
    "    expertise_tag = expertise_heading.find_next_sibling('p') if expertise_heading else None\n",
    "    research_expertise = expertise_tag.get_text(strip=True) if expertise_tag else \"\"\n",
    "\n",
    "    return {\n",
    "        'email': email,\n",
    "        'position': position,\n",
    "        'research_expertise': research_expertise\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "url = 'https://vcresearch.berkeley.edu/faculty/david-m-auslander'\n",
    "faculty_info = extract_faculty_info(url)\n",
    "\n",
    "print(\"Email:\", faculty_info['email'])\n",
    "print(\"Position:\", faculty_info['position'])\n",
    "print(\"Research Expertise:\", faculty_info['research_expertise'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-10T17:11:31.243958800Z",
     "start_time": "2024-07-10T17:11:30.517878Z"
    }
   },
   "id": "dce77d81e665bf1c",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email: dma@me.berkeley.edu\n",
      "Position: Professor of the Graduate School\n",
      "Research Expertise: control systems, simulation, mechatronics, real time software, energy management, satellite attitude control, demand response, machine control\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp\\ipykernel_11372\\1330418654.py:22: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  expertise_heading = soup.find('h2', text='Research Expertise and Interest')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_faculty_info(url):\n",
    "    # Fetch the HTML content from the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Extract email\n",
    "    email_tag = soup.find('a', href=lambda x: x and x.startswith('mailto:'))\n",
    "    email = email_tag.get_text(strip=True) if email_tag else \"\"\n",
    "\n",
    "    # Extract position\n",
    "    position_tag = soup.find('p', class_='large')\n",
    "    position = position_tag.get_text(strip=True) if position_tag else \"\"\n",
    "\n",
    "    # Extract research expertise\n",
    "    expertise_heading = soup.find('h2', text='Research Expertise and Interest')\n",
    "    expertise_tag = expertise_heading.find_next_sibling('p') if expertise_heading else None\n",
    "    research_expertise = expertise_tag.get_text(strip=True) if expertise_tag else \"\"\n",
    "    \n",
    "    # Clean up extra spaces in research expertise\n",
    "    research_expertise = ' '.join(research_expertise.split())\n",
    "\n",
    "    return {\n",
    "        'email': email,\n",
    "        'position': position,\n",
    "        'research_expertise': research_expertise\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "url = 'https://vcresearch.berkeley.edu/faculty/david-m-auslander'\n",
    "faculty_info = extract_faculty_info(url)\n",
    "\n",
    "print(\"Email:\", faculty_info['email'])\n",
    "print(\"Position:\", faculty_info['position'])\n",
    "print(\"Research Expertise:\", faculty_info['research_expertise'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-10T17:14:20.359698200Z",
     "start_time": "2024-07-10T17:14:19.251925300Z"
    }
   },
   "id": "10a3bd3137d1b319",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp\\ipykernel_11372\\52881759.py:23: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  expertise_heading = soup.find('h2', text='Research Expertise and Interest')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping and CSV update complete.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_data(url):\n",
    "    # Fetch the HTML content from the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract position\n",
    "    position_tag = soup.find('p', class_='large')\n",
    "    position = position_tag.get_text(strip=True) if position_tag else \"\"\n",
    "\n",
    "    # Extract email\n",
    "    email_tag = soup.find('a', href=lambda x: x and x.startswith('mailto:'))\n",
    "    email = email_tag.get_text(strip=True) if email_tag else \"\"\n",
    "\n",
    "    # Extract research expertise\n",
    "    expertise_heading = soup.find('h2', text='Research Expertise and Interest')\n",
    "    expertise_tag = expertise_heading.find_next_sibling('p') if expertise_heading else None\n",
    "    research_expertise = expertise_tag.get_text(strip=True) if expertise_tag else \"\"\n",
    "    \n",
    "    # Clean up extra spaces in research expertise\n",
    "    research_expertise = ' '.join(research_expertise.split())\n",
    "\n",
    "    return {\n",
    "        \"Position\": position,\n",
    "        \"Email\": email,\n",
    "        \"Research Focus\": research_expertise,\n",
    "    }\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file = r'D:\\Files\\Upwork\\Scrape\\Us_30_Uni_engineering\\Src\\4_Berkeley\\MechanicalEngineering\\me.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Initialize new columns in the DataFrame\n",
    "df['Position'] = ''\n",
    "df['Email'] = ''\n",
    "df['Research Focus'] = ''\n",
    "\n",
    "url = 'https://vcresearch.berkeley.edu/faculty/david-m-auslander'\n",
    "# Loop through URLs and scrape data\n",
    "for index, row in df.iterrows():\n",
    "    url = row['Link']  # Assuming the column containing URLs is named 'Link'\n",
    "    scraped_data = scrape_data(url)\n",
    "    df.at[index, 'Position'] = scraped_data[\"Position\"]\n",
    "    df.at[index, 'Email'] = scraped_data[\"Email\"]\n",
    "    df.at[index, 'Research Focus'] = scraped_data[\"Research Focus\"]\n",
    "\n",
    "# Save the updated DataFrame back to the CSV\n",
    "output_csv_file = 'Mechanical_result.csv'\n",
    "df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "print(\"Scraping and CSV update complete.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-10T19:39:54.556096900Z",
     "start_time": "2024-07-10T19:39:17.624699600Z"
    }
   },
   "id": "ab916f6dd62507e0",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered CSV file has been saved as 'filtered_profiles_urls.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(r'D:\\Files\\Upwork\\Scrape\\Us_30_Uni_engineering\\Src\\4_Berkeley\\profiles_urls.csv')\n",
    "\n",
    "# Filter rows that contain \"engineering\" in the \"Department\" column (case insensitive)\n",
    "filtered_df = df[df['Department'].str.contains('engineering', case=False, na=False)]\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_df.to_csv('filtered_profiles_urls.csv', index=False)\n",
    "\n",
    "print(\"Filtered CSV file has been saved as 'filtered_profiles_urls.csv'.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-10T20:15:49.359619800Z",
     "start_time": "2024-07-10T20:15:49.260348400Z"
    }
   },
   "id": "a1289800fd4a6842",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "97882e214273e7ab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
